{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy import shape\n",
    "import pandas as pd\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import open_clip\n",
    "import logging\n",
    "import traceback\n",
    "from blip.models.blip import blip_decoder, BLIP_Decoder\n",
    "from PIL import Image,ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "import string\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import inspect\n",
    "import torch.nn.functional as F\n",
    "import codecs\n",
    "import unicodedata\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import albumentations as A\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "def rm_punct(s)->str:\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))\n",
    "def rm_white(s)->str:\n",
    "    return  \" \".join(s.split())\n",
    "def rm_sw(s)->str:\n",
    "    sw = set(stopwords.words(\"english\"))\n",
    "    wt = word_tokenize(s)\n",
    "    t = ' '.join(w for w in wt if w not in sw)\n",
    "    return t\n",
    "def rm_spec(s)->str:\n",
    "    return re.sub('\\s+', ' ', (re.sub('_', '', (re.sub('[^a-zA-z0-9\\s]', '', unidecode(s)))))).strip().lower()\n",
    "def w_ec(s):\n",
    "    return d.check(str(s))\n",
    "\n",
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = \"P:/sd_prompts/images/\"\n",
    "    caption_path = image_path\n",
    "    tags_save_path = image_path\n",
    "    tag_list_path = \"P:/sd_prompts/text_tags/\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_name = 'ViT-H-14/laion2b_s32b_b79k'\n",
    "    image_embedding = 2048\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 2048\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    sentence_model_path = \"E:/j_diff/ci/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embedding_length = 384\n",
    "\n",
    "    blip_model: BLIP_Decoder = None\n",
    "    blip_image_eval_size: int = 384\n",
    "    blip_accuracy_confidence: float = 0.7\n",
    "    blip_max_cap_len: int = 120\n",
    "    blip_min_cap_len: int = 20\n",
    "    blip_rep_pen: float = 1.2\n",
    "    blip_model_type: str = 'large'\n",
    "    blip_model_url: str = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\n",
    "    blip_num_beams: int = 1\n",
    "    blip_offload: bool = False\n",
    "    clip_model_name: str = 'ViT-H-14/laion2b_s32b_b79k'\n",
    "    clip_model_path: str = None\n",
    "    clip_model = None\n",
    "    clip_preprocess = None\n",
    "    clip_sensitivity: int = 30\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    block_size = 2048\n",
    "    clip_model_list = \\\n",
    "                ['ViT-B-32/openai', #                  #       0\n",
    "                'ViT-B-32/laion400m_e31',#             #       1\n",
    "                'ViT-B-32/laion400m_e32',#             #       2\n",
    "                'ViT-B-32/laion2b_e16',#               #       3\n",
    "                'ViT-B-32/laion2b_s34b_b79k',#         #       4\n",
    "                'ViT-B-32-quickgelu/openai',#          #       5\n",
    "                'ViT-B-32-quickgelu/laion400m_e31',#   #       6\n",
    "                'ViT-B-32-quickgelu/laion400m_e32',#   #       7\n",
    "                'ViT-B-16/openai',#                    #       8\n",
    "                'ViT-B-16/laion400m_e31',#             #       9\n",
    "                'ViT-B-16/laion400m_e32',#             #       10\n",
    "                'ViT-B-16-plus-240/laion400m_e31',#    #       11\n",
    "                'ViT-B-16-plus-240/laion400m_e32',#    #       12\n",
    "                'ViT-L-14/openai',#                    #       13\n",
    "                'ViT-L-14/laion400m_e31',#             #       14\n",
    "                'ViT-L-14/laion400m_e32',#             #       15\n",
    "                'ViT-L-14/laion2b_s32b_b82k',#     `   #       16\n",
    "                'ViT-L-14-336/openai',#                #       17\n",
    "                'ViT-H-14/laion2b_s32b_b79k',#         #       18\n",
    "                'ViT-g-14/laion2b_s12b_b42k',#         #       19\n",
    "                'clip-vit-large-patch14-336/openai']#  #       20\n",
    "\n",
    "st_model = SentenceTransformer(CFG.sentence_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.blip_model is None:\n",
    "    print(\"BLIP\")\n",
    "    blip_path = os.path.dirname(inspect.getfile(blip_decoder))\n",
    "    configs_path = os.path.join(os.path.dirname(blip_path), 'configs')\n",
    "    med_config = os.path.join(configs_path, 'med_config.json')\n",
    "    blip_model = blip_decoder(\n",
    "        pretrained=CFG.blip_model_url,\n",
    "        vit='large',\n",
    "        med_config=med_config\n",
    "    )\n",
    "    blip_model.eval()\n",
    "    blip_model = blip_model.to(CFG.device)\n",
    "    blip_model = blip_model\n",
    "else:\n",
    "    blip_model = CFG.blip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.clip_model is None:\n",
    "    clip_model_name, clip_model_pretrained_name = CFG.clip_model_list[16].split('/', 2)\n",
    "    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "        clip_model_name, \n",
    "        pretrained = clip_model_pretrained_name, \n",
    "        precision='fp16' if CFG.device == 'cuda' else 'fp32',\n",
    "        device = CFG.device,\n",
    "        jit=False,\n",
    "        cache_dir = CFG.clip_model_path\n",
    "    )\n",
    "    clip_model.to(CFG.device).eval()\n",
    "    clip_tokenize = open_clip.get_tokenizer(clip_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_image_datum = []\n",
    "tag_list         = []\n",
    "cat_list           = []\n",
    "tag_path_files = [  f for f \n",
    "                    in os.listdir(CFG.tag_list_path[:-1:]) \n",
    "                    if os.path.isfile(CFG.tag_list_path+f)]\n",
    "tag_text_files = [  CFG.tag_list_path+f for f \n",
    "                    in tag_path_files \n",
    "                    if f[-(f[::-1].find('.')):] \n",
    "                    in ['txt','cap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_proc(tags):\n",
    "    blocks = np.array_split(tags, max(1, (len(tags)/CFG.block_size)))\n",
    "    block_embeds = []\n",
    "    for block in tqdm(blocks):\n",
    "        text_tokens = clip_tokenize(block).to(CFG.device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            text_features = clip_model.encode_text(text_tokens)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features.detach().half().cpu().numpy()\n",
    "        for i in range(text_features.shape[0]):\n",
    "            block_embeds.append(text_features[i])\n",
    "        cat_embed = torch.stack([torch.from_numpy(t) for t in block_embeds]).to(CFG.device)\n",
    "        cat_list.append(cat_embed)\n",
    "    return cat_list,tags\n",
    "\n",
    "def tag_blocks():\n",
    "    for tag_file in tqdm(tag_text_files,desc=\"Tag Files\"):\n",
    "        tags = []\n",
    "        tags.clear()\n",
    "        with open(tag_file,\"rt\",encoding='utf-8') as me:\n",
    "            txt_data = bytes(me.read(),'utf-8')\n",
    "            txt_data = unidecode(codecs.decode(txt_data))\n",
    "            str_sanity = str(txt_data).split('\\n')\n",
    "            tags = [    str(rm_punct(rm_spec(rm_white(x)))).strip() for x \n",
    "                            in str_sanity]\n",
    "        block_proc(tags)\n",
    "        for x in tags:\n",
    "            tag_list.append(x)\n",
    "    return cat_list,tag_list\n",
    "\n",
    "cat_list,tag_list = tag_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cap(pil_image,imagefile)->list:\n",
    "    try:    \n",
    "        captions = \"\"\n",
    "        sens_list = [0.05,0.1,0.15]\n",
    "        caption_list = []\n",
    "        size = CFG.blip_image_eval_size\n",
    "        np_pil_image = np.array(pil_image)\n",
    "        b_img = transforms.Compose([\n",
    "            transforms.Resize((size,size),interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "            ])\n",
    "        with torch.no_grad():\n",
    "            ti = b_img(pil_image).unsqueeze(0).to(CFG.device)\n",
    "            for b_sen in tqdm(sens_list):\n",
    "                for b_len in range(10,30,5):\n",
    "                    captions =blip_model.generate(\n",
    "                                ti, \n",
    "                                sample = False, \n",
    "                                top_p = b_sen,\n",
    "                                max_length = CFG.blip_max_cap_len, \n",
    "                                min_length = b_len, \n",
    "                                repetition_penalty = 1.4,\n",
    "                                num_beams = CFG.blip_num_beams\n",
    "                                )                    \n",
    "                    captions = str(captions[0])\n",
    "                    captions = rm_sw(captions)\n",
    "                    captions = rm_punct(captions)\n",
    "                    captions = rm_white(captions)\n",
    "                    captions = str(captions) +'\\n'\n",
    "                    print(captions)\n",
    "                    caption_list.append(captions)\n",
    "        f_name = os.path.basename(imagefile)\n",
    "        with open(CFG.tags_save_path+f_name[:(len(f_name))-1-len(f_name[-(f_name[::-1].find('.')):])]+\"_captions\"+\".txt\",'wt') as fi:\n",
    "            fi.writelines(caption_list)\n",
    "    except Exception as e:\n",
    "            logging.error(traceback.format_exc())            \n",
    "    finally:\n",
    "        pass    \n",
    "    return caption_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip_clip_cap_tag(imagefile,cat_list):\n",
    "    try:\n",
    "        img = cv2.imread(imagefile)\n",
    "        img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(img2)\n",
    "        match_list = []\n",
    "        match_list.clear()\n",
    "        val_list = []\n",
    "        val_list.clear()\n",
    "        uniq_match = []\n",
    "        uniq_match.clear()\n",
    "        matches = []\n",
    "        matches.clear()        \n",
    "        caption = get_cap(pil_image,imagefile)\n",
    "        for tags in cat_list:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                    image = clip_preprocess(pil_image).unsqueeze(0).to(CFG.device)\n",
    "                    image_features_n = clip_model.encode_image(image)\n",
    "                    image_features_n =  F.normalize(image_features_n,dim=-1)\n",
    "                    text_embeddings_n = F.normalize(kitty, dim=-1)\n",
    "                    dot_similarity =  image_features_n @ text_embeddings_n.T\n",
    "                    values, indices = dot_similarity.float().cpu().topk(k=1,largest=True, sorted=True)\n",
    "                    indices.detach().float().cpu().numpy()\n",
    "                    values.detach().float().cpu()\n",
    "                    val_list = [abs(int(torch.IntTensor.item(x)*100)) for x in values[0,:][::]]\n",
    "                    matches = [tags[x] for x in indices[0,:][::]]\n",
    "                    for i in range(len(val_list)):\n",
    "                        if val_list[i]>=CFG.clip_sensitivity:\n",
    "                            match_list.append(str(val_list[i])+'_'+str(matches[i]))\n",
    "                    values.detach().float().cpu().numpy()\n",
    "                    del indices\n",
    "                    del values\n",
    "                    del val_list\n",
    "                    del matches\n",
    "        print(len(match_list))\n",
    "        print(match_list)\n",
    "        # f_name = os.path.basename(imagefile)\n",
    "        # if os.path.isfile(CFG.tags_save_path+f_name[:(len(f_name))-1-len(f_name[-(f_name[::-1].find('.')):])]+\".txt\"):\n",
    "        #     with open(CFG.tags_save_path+f_name[:(len(f_name))-1-len(f_name[-(f_name[::-1].find('.')):])]+\".txt\",'a') as fi:\n",
    "        #         for x in match_list:\n",
    "        #             fi.write(str(x)+'\\n')\n",
    "        # elif not os.path.isfile(CFG.tags_save_path+f_name[:(len(f_name))-1-len(f_name[-(f_name[::-1].find('.')):])]+\".txt\"):\n",
    "        #     with open(CFG.tags_save_path+f_name[:(len(f_name))-1-len(f_name[-(f_name[::-1].find('.')):])]+\".txt\",'wt') as fi:\n",
    "        #         for x in match_list:\n",
    "        #             fi.write(str(x)+'\\n')\n",
    "    except Exception as e:\n",
    "        logging.error(traceback.format_exc())            \n",
    "    finally:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def img_tag_proc(img_list):\n",
    "#     with ThreadPoolExecutor(8) as executor:\n",
    "#         futures = [executor.submit(blip_clip_cap_tag, i) for i in img_list]\n",
    "#         for _ in as_completed(futures):\n",
    "#             status_bar.update(n=1)\n",
    "\n",
    "\n",
    "# imgs_len = len(image_list)\n",
    "# status_bar = tqdm(total=imgs_len, desc='Image Processing')\n",
    "for imagefile in tqdm(image_list):\n",
    "    blip_clip_cap_tag(imagefile,cat_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
